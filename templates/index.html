<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>CLIP Recommendation</title>
    <style>
    .center {
        padding-top: 25px;
        text-align: center;
        margin: auto;
    }
    
    .imageBox {
        margin: 5px;
        float: left;
        width: 320px;
        height: 600px;
    }
    
    div.imageBox img {
        width: 100%;
        height: auto;
    }
    
    .desc {
        padding: 15px;
        text-align: center;
    }</style>
</head>
<body>
    <div class="center">
        <form method="post", enctype="multipart/form-data">
            <input type="text" name="text_input" value="">
            <br></br>
            <input type="file" id="image-input" name="image">
            <br></br>
            <input type="submit" value="Find Image">
        </form>
    </div>
    <div>
        <h1 id="description">Description</h1>
        <h2 id="functionality">Functionality</h2>
        <p>This app allows uploading an image or the text, and then uses a CLIP (Contrastive Language-Image Pre-Training) neural network to browse our selection of thousands of photos to find pictures that are similar to the uploaded file or the provided image description. The application suggests 20 answers which have the highest similarity score to the input.</p>
        <h2 id="ui">UI</h2>
        <p>The UI is a dynamic web page hosted on the <a href="https://cloud.google.com/compute/docs/containers">Google Cloud Container</a>. The content of web pages won't always remain the same for all users, as we use the API and the model to change the content of the webpage based on users input. </p>
        <h2 id="storage">Storage</h2>
        <p>The images are present inside of the <a href="https://cloud.google.com/storage">Google Cloud Storage</a> and all of them are available from the internet. The images are the subset of the <a href="https://cocodataset.org/#download">COCO Dataset</a>. Rest of the files are present in the Docker Container, including the CLIP model,  <code>index.pkl</code> file (which holds the images and texts from the database vectorized by CLIP and saved by faiss), and the <code>filenames.txt</code> which includes the images names.</p>
        <h2 id="processing">Processing</h2>
        <p>The OpenAI <a href="https://github.com/openai/CLIP">CLIP</a> is dockerized with the rest of the application (hidden data + frontend) and deployed using <a href="https://cloud.google.com/run">Cloud Run</a>. The application is purely asynchronous as for the communication with the model and calling the images we are using a FastAPI.</p>
        <p>The <a href="https://fastapi.tiangolo.com">FastAPI</a> is built on top of the ASGI (Asynchronous Server Gateway Interface) framework, which is designed to handle asynchronous code efficiently. Moreover it uses the async/await syntax to define asynchronous endpoints and functions.</p>
        <p>CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. The CLIP matches the performance of the original ResNet50 on ImageNet “zero-shot” without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision. The model:</p>
        <ul>
        <li>Given a batch of images, returns the image features encoded by the vision portion of the CLIP model,</li>
        <li>Given a batch of text tokens, returns the text features encoded by the language portion of the CLIP model,</li>
        <li>Given a batch of images and a batch of text tokens, returns two Tensors, containing the logit scores corresponding to each image and text input. The values are cosine similarities between the corresponding image and text features, times 100.</li>
        </ul>
    </div>
</body>
</html>